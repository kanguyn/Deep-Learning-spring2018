{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 1.5 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Group name:</b> Deeplearn 38 - Albane Bonnaud & Kim-Anh-Nhi Nguyen\n",
    " \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Generate your final report (export as HTML) and upload it on the submission website http://bigfoot-m1.eurecom.fr/teachingsub/login (using your deeplearnXX/password). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed and submitted by May 30th 2018 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%. Can you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks,  **LeNet-5**, to go to more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell below to load the MNIST data that comes with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "epsilon = 1e-10 # this is a parameter you will use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example\n",
    "**y=softmax(Wx+b)** seen in the first lab. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the TensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.288166094\n",
      "Epoch:  02   =====> Loss= 0.732756539\n",
      "Epoch:  03   =====> Loss= 0.600399079\n",
      "Epoch:  04   =====> Loss= 0.536668742\n",
      "Epoch:  05   =====> Loss= 0.497883010\n",
      "Epoch:  06   =====> Loss= 0.471130213\n",
      "Epoch:  07   =====> Loss= 0.451441533\n",
      "Epoch:  08   =====> Loss= 0.435906629\n",
      "Epoch:  09   =====> Loss= 0.423306856\n",
      "Epoch:  10   =====> Loss= 0.413005027\n",
      "Epoch:  11   =====> Loss= 0.404336038\n",
      "Epoch:  12   =====> Loss= 0.396785073\n",
      "Epoch:  13   =====> Loss= 0.390349418\n",
      "Epoch:  14   =====> Loss= 0.384353908\n",
      "Epoch:  15   =====> Loss= 0.379181639\n",
      "Epoch:  16   =====> Loss= 0.374695726\n",
      "Epoch:  17   =====> Loss= 0.370589093\n",
      "Epoch:  18   =====> Loss= 0.366279961\n",
      "Epoch:  19   =====> Loss= 0.362964873\n",
      "Epoch:  20   =====> Loss= 0.359622410\n",
      "Epoch:  21   =====> Loss= 0.356603251\n",
      "Epoch:  22   =====> Loss= 0.353947618\n",
      "Epoch:  23   =====> Loss= 0.351042153\n",
      "Epoch:  24   =====> Loss= 0.348606701\n",
      "Epoch:  25   =====> Loss= 0.346464758\n",
      "Epoch:  26   =====> Loss= 0.344220706\n",
      "Epoch:  27   =====> Loss= 0.342432736\n",
      "Epoch:  28   =====> Loss= 0.340255660\n",
      "Epoch:  29   =====> Loss= 0.338335244\n",
      "Epoch:  30   =====> Loss= 0.336661362\n",
      "Epoch:  31   =====> Loss= 0.335130091\n",
      "Epoch:  32   =====> Loss= 0.333227386\n",
      "Epoch:  33   =====> Loss= 0.332046482\n",
      "Epoch:  34   =====> Loss= 0.330370903\n",
      "Epoch:  35   =====> Loss= 0.329284512\n",
      "Epoch:  36   =====> Loss= 0.327516881\n",
      "Epoch:  37   =====> Loss= 0.326620072\n",
      "Epoch:  38   =====> Loss= 0.325195372\n",
      "Epoch:  39   =====> Loss= 0.324184005\n",
      "Epoch:  40   =====> Loss= 0.323181397\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9157\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h4>COMMENT:</h4>\n",
    "We just trained and tested the multilayer perceptron model using softmax.\n",
    "The accuracy we get is 0.9157.\n",
    "This is already a good accuray (above 90% !) but we're aiming a 99% accuracy, and this will be our challenge in section 2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=lab_2/log_files/\"**\n",
    "- Click on \"Tensorboard web interface\" in Zoe  \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now more familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](lenet.png \"Figure 1: Lenet-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><span>Figure 1: Lenet-5 </span></center>\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0., shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define LeNet-5 convolutional and max pool layers\n",
    "def conv2d(x, W,pad='SAME'):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=pad)\n",
    "# 2x2 max pooling layer\n",
    "def max_pool_size2(x,pad='VALID'):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model(data, keep_prob=1):\n",
    "    input_data = tf.reshape(data,[-1,28,28,1])    \n",
    "    \n",
    "    # Layer 1: Convolution / Input shape = 28x28x1 / Output shape = 28x28x6\n",
    "    w1 = weight_variable([5,5,1,6])\n",
    "    b1 = bias_variable([6])\n",
    "    conv1 = tf.nn.relu(conv2d(input_data , w1) + b1) #activation\n",
    "    \n",
    "    # Max Pooling / Input shape = 28x28x6 / Output shape = 14x14x6\n",
    "    pool1 = max_pool_size2(conv1)\n",
    "    \n",
    "    # Layer 2: Convolutional / Input shape = 14*14*6 / Output shape = 10x10x16\n",
    "    w2 = weight_variable([5,5,6,16])\n",
    "    b2 = bias_variable([16])\n",
    "    conv2 = tf.nn.relu(conv2d( pool1  ,  w2  ,  pad='VALID') + b2) # activation\n",
    "    \n",
    "    # Max Pooling / Input shape = 10x10x16 / Output shape = 5x5x16\n",
    "    pool2 = max_pool_size2(conv2)\n",
    "    \n",
    "    # Flatten the output shape / Input shape = 5x5x16 / Output shape = 400\n",
    "    pool2_flat = tf.contrib.layers.flatten(pool2)\n",
    "    \n",
    "    # Layer 3: Fully Connected / Input shape = 400 / Output shape = 120\n",
    "    w3 = weight_variable([400, 120])\n",
    "    b3 = bias_variable([120])\n",
    "    fc1 = tf.nn.relu(tf.matmul(pool2_flat, w3) + b3) # activation\n",
    "    \n",
    "    # Layer 4: Fully Connected / Input shape = 120 / Output shape = 84\n",
    "    w4 = weight_variable([120, 84])\n",
    "    b4 = bias_variable([84])\n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, w4) + b4) # activation\n",
    "    \n",
    "    # Layer 5: Fully Connected / Input shape = 84 / Output shape = 10\n",
    "    w5 = weight_variable([84, 10])\n",
    "    b5 = bias_variable([10])\n",
    "    \n",
    "    pred = tf.nn.softmax(tf.matmul(fc2, w5) + b5) # softmax activation\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in LeNet5: 61706\n"
     ]
    }
   ],
   "source": [
    "number_parameters_LeNet5 = (5*5*1 + 1)*6 + (5*5*6 + 1)*16 + (400 + 1)*120 + (120 + 1)*84 + (84 + 1)*10\n",
    "print('Number of parameters in LeNet5: {}'.format(number_parameters_LeNet5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3>Explanations:</h3>\n",
    "<ul>\n",
    "<li><b>The input layer</b> only reads the input images, so there is no parameter to learn in this layer.</li>\n",
    "\n",
    "<br>\n",
    "<li>For <b>the first convolutionnal layer</b> we have 1 filter of size 5 x 5. The output size is 6, so we learn 6 different 5x5x1 filters. There is also a bias term for each output, so we have a total number of parameters of:\n",
    "    <ul><li>Weights and biases for convolutional layer 1 = (5 x 5 x 1 +1) x 6</li></ul></li>\n",
    "<br>\n",
    "<li>The reasoning is the same for <b>the second convolutional layer</b>:\n",
    "    <ul><li>Weights and biases for convolutional layer 2 = (5 x 5 x 6 +1) x 16</li></ul></li>\n",
    "<br>\n",
    "<li>There is no parameter to learn in a <b>pooling layer</b>.</li>\n",
    "<br>\n",
    "<li>In a <b>fully-connected laye</b>, all input units have a separate weight to each output unit. For n inputs and m outputs, the number of weights is n x m. Additionally, there is a bias for each output node, so it comes to (n+1) x m parameters.\n",
    "    <ul><li>Weights and biases for fully connected layer 1 = (400 + 1) x 120</li>\n",
    "        <li>Weights and biases for fully connected layer 2 = (120 + 1) x 84</li></ul></li>\n",
    "<br>\n",
    "<li>The <b>output layer</b> is just a normal fully connected layer. The same formula applies.\n",
    "    <ul><li>Weights and biases for output layer = (84 + 1) x 10</li></ul></li>\n",
    "<br>\n",
    "</ul>\n",
    "<br>\n",
    "TOTAL number of parameters: 61706\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Fucntion: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_le_net ( modelName, model, learning_rate = 0.001, \n",
    "            training_epochs = 40, batch_size = 128, display_step = 1, keep_p = 1,\n",
    "            optimFunc = tf.train.GradientDescentOptimizer,\n",
    "            X_train=mnist.train.images, y_train=mnist.train.labels,\n",
    "            X_val=mnist.validation.images, y_val=mnist.validation.labels,\n",
    "            X_test= mnist.test.images,y_test=mnist.test.labels\n",
    "          ):\n",
    "    \n",
    "    InputSize , OutputSize, TrainingSetSize, ValidationSetSize, TestSetSize = X_train[0].shape[0], y_train[0].shape[0], len(X_train), len(X_validation), len(X_test)    \n",
    "    \n",
    "    logsFolder = 'log_files/' # useful for tensorboard\n",
    "    saveFolder = 'Models/'    # useful to restore the model\n",
    "    \n",
    "    tf.reset_default_graph() # reset graph\n",
    "\n",
    "    # tf Graph Input:  mnist data image of shape 28*28*1\n",
    "    x = tf.placeholder(tf.float32, [None,InputSize], name='InputData')\n",
    "    # 0-9 digits recognition,  10 classes\n",
    "    y = tf.placeholder(tf.float32, [None,OutputSize], name='LabelData')\n",
    "    # Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='DropoutKeepProbability')\n",
    "\n",
    "    with tf.name_scope('Model'):\n",
    "        # Model\n",
    "        model = model(x, keep_prob)\n",
    "    \n",
    "    with tf.name_scope('Loss'):\n",
    "        # Minimize error using cross entropy\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model+1e-9), reduction_indices=1))\n",
    "        #cost = tf.nn.softmax_cross_entropy_with_logits(model, tf.one_hot(y, 10))\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        #Optimization, using cost reduction\n",
    "        optimizer = optimFunc(learning_rate).minimize(cost)\n",
    "        \n",
    "    with tf.name_scope('Accuracy'):\n",
    "        # Accuracies\n",
    "        acc = let_net_accuracy(model, y)\n",
    "          \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "    # track accuracies\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"BatchLoss\", cost)\n",
    "    # Create a summary to monitor batch accuracy tensor\n",
    "    tf.summary.scalar(\"BatchAccuracy\", acc)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # train the model\n",
    "    train_le_net(model, cost, optimizer, acc,\n",
    "             x, y, keep_prob, TrainingSetSize,\n",
    "             X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "             init, merged_summary_op,\n",
    "             modelName, saveFolder,logsFolder,\n",
    "             learning_rate, training_epochs, batch_size, display_step , keep_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def let_net_accuracy(model, y):\n",
    "    accuracy = tf.reduce_mean(tf.cast( tf.equal( tf.argmax(model, 1), tf.argmax(y, 1)), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_le_net(model, cost, optimizer, acc,\n",
    "             x, y, keep_prob, TrainingSetSize,\n",
    "             X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "             init, merged_summary_op,\n",
    "             modelName, saveFolder, logsFolder,\n",
    "             learning_rate, training_epochs, batch_size, display_step, keep_p,\n",
    "             dataFileName=None):\n",
    "    \n",
    "    # Initial model print\n",
    "    print(\"*Model \", modelName,\" {learning rate: %.4f; nb of iterations: %d; batch size: %d}\"%\\\n",
    "          (learning_rate, training_epochs, batch_size))\n",
    "    \n",
    "    # Start a tensorflow session\n",
    "    with tf.Session() as sess:\n",
    "        print (\"   Start Training!\")\n",
    "        sess.run(init)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        \"\"\" # Load model if the parameter loadModel is not empty\n",
    "        if(loadModel):\n",
    "            print(loadModel)\n",
    "            saver.restore(sess=sess,save_path='Models/'+str(loadModel))\"\"\"\n",
    "       \n",
    "        \n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logsFolder,\n",
    "                                               graph=tf.get_default_graph())\n",
    "        \n",
    "        # Training cycle\n",
    "        t0 = time.time()\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                \n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                \"\"\"batch_xs, batch_ys = input_pipeline(filenames=dataFileName, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    num_epochs=total_batch)\"\"\"\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                          feed_dict={x: batch_xs,\n",
    "                                                     y: batch_ys,\n",
    "                                                     keep_prob: keep_p})\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "\n",
    "            # Display logs per epoch step\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                tr_acc = acc.eval({x: X_train, y: y_train, keep_prob: 1})\n",
    "                vl_acc = acc.eval({x: X_val, y: y_val, keep_prob: 1})\n",
    "                print(\"   Epoch: %02d | Loss=%.9f | Train accuracy=%.3f %% | Validation Accuracy=%.3f %%\"% \n",
    "                      (epoch+1, avg_cost, tr_acc*100, vl_acc*100));\n",
    "        \n",
    "        \n",
    "        print (\"   Training Finished in %.1f seconds.\"%(time.time()-t0))\n",
    "        \n",
    "        # Evaluating model with the accuracies\n",
    "        print (\"      \\nFinal accuracies:\\n\")\n",
    "        print (\"    * Train accuracy: %.3f %%\"%(100*acc.eval({x: X_train, y: y_train, keep_prob: 1})))\n",
    "        print (\"    * Validation accuracy: %.3f %%\"%(100*acc.eval({x: X_val, y: y_val, keep_prob: 1})))\n",
    "        print (\"    * Test accuracy: %.3f %%\"%(100*acc.eval({x: X_test, y: y_test, keep_prob: 1})))\n",
    "        \n",
    "        # Saving Model\n",
    "        saver.save(sess=sess,save_path=saveFolder+modelName)\n",
    "        print (\"   Saving model in file: %s\"%(saveFolder+modelName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model  lenet5-model-SGD-lr=0.001  {learning rate: 0.0010; nb of iterations: 40; batch size: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=2.290001814 | Train accuracy=26.418 % | Validation Accuracy=26.340 %\n",
      "   Epoch: 02 | Loss=2.227923595 | Train accuracy=40.733 % | Validation Accuracy=41.720 %\n",
      "   Epoch: 03 | Loss=2.109430395 | Train accuracy=48.500 % | Validation Accuracy=48.500 %\n",
      "   Epoch: 04 | Loss=1.818057649 | Train accuracy=61.491 % | Validation Accuracy=61.740 %\n",
      "   Epoch: 05 | Loss=1.313544558 | Train accuracy=75.755 % | Validation Accuracy=75.920 %\n",
      "   Epoch: 06 | Loss=0.865026968 | Train accuracy=81.978 % | Validation Accuracy=82.220 %\n",
      "   Epoch: 07 | Loss=0.619916314 | Train accuracy=85.055 % | Validation Accuracy=85.700 %\n",
      "   Epoch: 08 | Loss=0.503556971 | Train accuracy=86.871 % | Validation Accuracy=87.540 %\n",
      "   Epoch: 09 | Loss=0.443044210 | Train accuracy=87.973 % | Validation Accuracy=88.400 %\n",
      "   Epoch: 10 | Loss=0.405815024 | Train accuracy=88.829 % | Validation Accuracy=89.220 %\n",
      "   Epoch: 11 | Loss=0.372745791 | Train accuracy=89.436 % | Validation Accuracy=89.800 %\n",
      "   Epoch: 12 | Loss=0.356278223 | Train accuracy=89.955 % | Validation Accuracy=90.380 %\n",
      "   Epoch: 13 | Loss=0.337102807 | Train accuracy=90.433 % | Validation Accuracy=90.700 %\n",
      "   Epoch: 14 | Loss=0.321525146 | Train accuracy=90.722 % | Validation Accuracy=91.020 %\n",
      "   Epoch: 15 | Loss=0.305116867 | Train accuracy=91.145 % | Validation Accuracy=91.380 %\n",
      "   Epoch: 16 | Loss=0.297600901 | Train accuracy=91.484 % | Validation Accuracy=91.800 %\n",
      "   Epoch: 17 | Loss=0.285439707 | Train accuracy=91.640 % | Validation Accuracy=91.740 %\n",
      "   Epoch: 18 | Loss=0.276078782 | Train accuracy=92.007 % | Validation Accuracy=92.120 %\n",
      "   Epoch: 19 | Loss=0.263770411 | Train accuracy=92.211 % | Validation Accuracy=92.600 %\n",
      "   Epoch: 20 | Loss=0.258162986 | Train accuracy=92.409 % | Validation Accuracy=92.760 %\n",
      "   Epoch: 21 | Loss=0.249543069 | Train accuracy=92.660 % | Validation Accuracy=93.100 %\n",
      "   Epoch: 22 | Loss=0.243467834 | Train accuracy=92.833 % | Validation Accuracy=93.280 %\n",
      "   Epoch: 23 | Loss=0.236387095 | Train accuracy=93.044 % | Validation Accuracy=93.480 %\n",
      "   Epoch: 24 | Loss=0.229692009 | Train accuracy=93.287 % | Validation Accuracy=93.760 %\n",
      "   Epoch: 25 | Loss=0.221535586 | Train accuracy=93.464 % | Validation Accuracy=93.880 %\n",
      "   Epoch: 26 | Loss=0.218050125 | Train accuracy=93.576 % | Validation Accuracy=94.040 %\n",
      "   Epoch: 27 | Loss=0.212030310 | Train accuracy=93.691 % | Validation Accuracy=94.280 %\n",
      "   Epoch: 28 | Loss=0.206054913 | Train accuracy=93.844 % | Validation Accuracy=94.420 %\n",
      "   Epoch: 29 | Loss=0.202517239 | Train accuracy=94.016 % | Validation Accuracy=94.560 %\n",
      "   Epoch: 30 | Loss=0.197936200 | Train accuracy=94.105 % | Validation Accuracy=94.680 %\n",
      "   Epoch: 31 | Loss=0.193878595 | Train accuracy=94.227 % | Validation Accuracy=94.760 %\n",
      "   Epoch: 32 | Loss=0.189830385 | Train accuracy=94.413 % | Validation Accuracy=94.960 %\n",
      "   Epoch: 33 | Loss=0.183225765 | Train accuracy=94.536 % | Validation Accuracy=94.960 %\n",
      "   Epoch: 34 | Loss=0.184128982 | Train accuracy=94.636 % | Validation Accuracy=95.100 %\n",
      "   Epoch: 35 | Loss=0.175176273 | Train accuracy=94.691 % | Validation Accuracy=95.040 %\n",
      "   Epoch: 36 | Loss=0.175766158 | Train accuracy=94.771 % | Validation Accuracy=95.360 %\n",
      "   Epoch: 37 | Loss=0.170828144 | Train accuracy=94.947 % | Validation Accuracy=95.300 %\n",
      "   Epoch: 38 | Loss=0.169076984 | Train accuracy=95.042 % | Validation Accuracy=95.580 %\n",
      "   Epoch: 39 | Loss=0.165470636 | Train accuracy=95.125 % | Validation Accuracy=95.600 %\n",
      "   Epoch: 40 | Loss=0.160400489 | Train accuracy=95.215 % | Validation Accuracy=95.840 %\n",
      "   Training Finished in 1365.2 seconds.\n",
      "      \n",
      "Final accuracies:\n",
      "\n",
      "    * Train accuracy: 95.215 %\n",
      "    * Validation accuracy: 95.840 %\n",
      "    * Test accuracy: 95.390 %\n",
      "   Saving model in file: Models/lenet5-model-SGD-lr=0.001\n"
     ]
    }
   ],
   "source": [
    "apply_le_net (model=LeNet5_Model, modelName ='lenet5-model-SGD-lr=0.001',\n",
    "           learning_rate = 0.001, \n",
    "           training_epochs = 40,\n",
    "           batch_size = 128, \n",
    "           optimFunc = tf.train.GradientDescentOptimizer\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model  lenet5-model-SGD-lr=0.01  {learning rate: 0.0100; nb of iterations: 40; batch size: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=1.422941481 | Train accuracy=86.687 % | Validation Accuracy=87.400 %\n",
      "   Epoch: 02 | Loss=0.369122338 | Train accuracy=91.024 % | Validation Accuracy=91.340 %\n",
      "   Epoch: 03 | Loss=0.254931824 | Train accuracy=93.400 % | Validation Accuracy=93.660 %\n",
      "   Epoch: 04 | Loss=0.201007217 | Train accuracy=94.753 % | Validation Accuracy=95.060 %\n",
      "   Epoch: 05 | Loss=0.167994749 | Train accuracy=95.029 % | Validation Accuracy=95.440 %\n",
      "   Epoch: 06 | Loss=0.147176167 | Train accuracy=95.984 % | Validation Accuracy=96.240 %\n",
      "   Epoch: 07 | Loss=0.131061102 | Train accuracy=96.384 % | Validation Accuracy=96.560 %\n",
      "   Epoch: 08 | Loss=0.118119375 | Train accuracy=96.695 % | Validation Accuracy=96.800 %\n",
      "   Epoch: 09 | Loss=0.108751074 | Train accuracy=96.924 % | Validation Accuracy=96.940 %\n",
      "   Epoch: 10 | Loss=0.102593396 | Train accuracy=97.164 % | Validation Accuracy=97.340 %\n",
      "   Epoch: 11 | Loss=0.094723196 | Train accuracy=97.244 % | Validation Accuracy=97.140 %\n",
      "   Epoch: 12 | Loss=0.088361474 | Train accuracy=97.365 % | Validation Accuracy=97.460 %\n",
      "   Epoch: 13 | Loss=0.084437764 | Train accuracy=97.485 % | Validation Accuracy=97.480 %\n",
      "   Epoch: 14 | Loss=0.079092474 | Train accuracy=97.796 % | Validation Accuracy=97.780 %\n",
      "   Epoch: 15 | Loss=0.076529274 | Train accuracy=97.858 % | Validation Accuracy=97.840 %\n",
      "   Epoch: 16 | Loss=0.071852932 | Train accuracy=97.785 % | Validation Accuracy=97.560 %\n",
      "   Epoch: 17 | Loss=0.069231074 | Train accuracy=98.020 % | Validation Accuracy=97.940 %\n",
      "   Epoch: 18 | Loss=0.066358981 | Train accuracy=98.167 % | Validation Accuracy=97.980 %\n",
      "   Epoch: 19 | Loss=0.064215227 | Train accuracy=98.038 % | Validation Accuracy=97.720 %\n",
      "   Epoch: 20 | Loss=0.062294461 | Train accuracy=98.285 % | Validation Accuracy=98.040 %\n",
      "   Epoch: 21 | Loss=0.059535860 | Train accuracy=98.213 % | Validation Accuracy=98.040 %\n",
      "   Epoch: 22 | Loss=0.058470224 | Train accuracy=98.385 % | Validation Accuracy=98.220 %\n",
      "   Epoch: 23 | Loss=0.055195203 | Train accuracy=98.422 % | Validation Accuracy=98.260 %\n",
      "   Epoch: 24 | Loss=0.053292239 | Train accuracy=98.542 % | Validation Accuracy=98.260 %\n",
      "   Epoch: 25 | Loss=0.052909030 | Train accuracy=98.435 % | Validation Accuracy=98.160 %\n",
      "   Epoch: 26 | Loss=0.050326951 | Train accuracy=98.576 % | Validation Accuracy=98.340 %\n",
      "   Epoch: 27 | Loss=0.049387151 | Train accuracy=98.315 % | Validation Accuracy=98.000 %\n",
      "   Epoch: 28 | Loss=0.049129880 | Train accuracy=98.698 % | Validation Accuracy=98.360 %\n",
      "   Epoch: 29 | Loss=0.046097071 | Train accuracy=98.682 % | Validation Accuracy=98.380 %\n",
      "   Epoch: 30 | Loss=0.045063888 | Train accuracy=98.705 % | Validation Accuracy=98.400 %\n",
      "   Epoch: 31 | Loss=0.044905512 | Train accuracy=98.715 % | Validation Accuracy=98.480 %\n",
      "   Epoch: 32 | Loss=0.042936246 | Train accuracy=98.805 % | Validation Accuracy=98.380 %\n",
      "   Epoch: 33 | Loss=0.042515811 | Train accuracy=98.622 % | Validation Accuracy=98.320 %\n",
      "   Epoch: 34 | Loss=0.041174549 | Train accuracy=98.773 % | Validation Accuracy=98.400 %\n",
      "   Epoch: 35 | Loss=0.038527644 | Train accuracy=98.702 % | Validation Accuracy=98.280 %\n",
      "   Epoch: 36 | Loss=0.038899159 | Train accuracy=98.940 % | Validation Accuracy=98.500 %\n",
      "   Epoch: 37 | Loss=0.038839934 | Train accuracy=98.978 % | Validation Accuracy=98.400 %\n",
      "   Epoch: 38 | Loss=0.036500584 | Train accuracy=98.809 % | Validation Accuracy=98.420 %\n",
      "   Epoch: 39 | Loss=0.036522362 | Train accuracy=98.835 % | Validation Accuracy=98.260 %\n",
      "   Epoch: 40 | Loss=0.035499595 | Train accuracy=99.015 % | Validation Accuracy=98.460 %\n",
      "   Training Finished in 1568.7 seconds.\n",
      "      \n",
      "Final accuracies:\n",
      "\n",
      "    * Train accuracy: 99.015 %\n",
      "    * Validation accuracy: 98.460 %\n",
      "    * Test accuracy: 98.530 %\n",
      "   Saving model in file: Models/lenet5-model-SGD-lr=0.01\n"
     ]
    }
   ],
   "source": [
    "apply_le_net (model=LeNet5_Model, modelName ='lenet5-model-SGD-lr=0.01',\n",
    "           learning_rate = 0.01, \n",
    "           training_epochs = 40,\n",
    "           batch_size = 128,\n",
    "           optimFunc = tf.train.GradientDescentOptimizer\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please put your loss and accuracy curves here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Here are the curves of the accuracy and the loss obtained from Tensorboard:**\n",
    "### With `learning_rate = 0.001`\n",
    "![alt text](MNIST_figures/final-SGD-001.png \"Tensorboard curves - GradientDescent-0.001\")\n",
    "### With `learning_rate = 0.01`\n",
    "![alt text](MNIST_figures/final-SGD-01.png \"Tensorboard curves - GradientDescent-0.01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.1 </b>\n",
    "\n",
    "---\n",
    "- Retrain your network with AdamOptimizer and then fill the table\n",
    "- Which optimizer gives the best accuracy on test data?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model  lenet5-model-ADAM-lr=0.001  {learning rate: 0.0010; nb of iterations: 40; batch size: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=0.364301915 | Train accuracy=96.464 % | Validation Accuracy=96.920 %\n",
      "   Epoch: 02 | Loss=0.095402370 | Train accuracy=97.407 % | Validation Accuracy=97.680 %\n",
      "   Epoch: 03 | Loss=0.066976675 | Train accuracy=98.169 % | Validation Accuracy=97.960 %\n",
      "   Epoch: 04 | Loss=0.052581351 | Train accuracy=97.922 % | Validation Accuracy=97.740 %\n",
      "   Epoch: 05 | Loss=0.044239944 | Train accuracy=98.820 % | Validation Accuracy=98.620 %\n",
      "   Epoch: 06 | Loss=0.037465910 | Train accuracy=99.247 % | Validation Accuracy=98.880 %\n",
      "   Epoch: 07 | Loss=0.030688409 | Train accuracy=98.911 % | Validation Accuracy=98.380 %\n",
      "   Epoch: 08 | Loss=0.028008268 | Train accuracy=99.145 % | Validation Accuracy=98.800 %\n",
      "   Epoch: 09 | Loss=0.025486586 | Train accuracy=99.047 % | Validation Accuracy=98.760 %\n",
      "   Epoch: 10 | Loss=0.022684926 | Train accuracy=99.151 % | Validation Accuracy=98.800 %\n",
      "   Epoch: 11 | Loss=0.020156019 | Train accuracy=99.533 % | Validation Accuracy=99.000 %\n",
      "   Epoch: 12 | Loss=0.017256173 | Train accuracy=99.678 % | Validation Accuracy=98.980 %\n",
      "   Epoch: 13 | Loss=0.013983905 | Train accuracy=99.624 % | Validation Accuracy=98.960 %\n",
      "   Epoch: 14 | Loss=0.013473505 | Train accuracy=99.460 % | Validation Accuracy=98.820 %\n",
      "   Epoch: 15 | Loss=0.014435367 | Train accuracy=99.529 % | Validation Accuracy=98.760 %\n",
      "   Epoch: 16 | Loss=0.011271450 | Train accuracy=99.718 % | Validation Accuracy=98.940 %\n",
      "   Epoch: 17 | Loss=0.011004339 | Train accuracy=99.651 % | Validation Accuracy=99.020 %\n",
      "   Epoch: 18 | Loss=0.007324294 | Train accuracy=99.704 % | Validation Accuracy=98.960 %\n",
      "   Epoch: 19 | Loss=0.008317061 | Train accuracy=99.835 % | Validation Accuracy=98.920 %\n",
      "   Epoch: 20 | Loss=0.008414551 | Train accuracy=99.716 % | Validation Accuracy=98.880 %\n",
      "   Epoch: 21 | Loss=0.009597751 | Train accuracy=99.764 % | Validation Accuracy=98.820 %\n",
      "   Epoch: 22 | Loss=0.010361257 | Train accuracy=99.647 % | Validation Accuracy=98.840 %\n",
      "   Epoch: 23 | Loss=0.005023222 | Train accuracy=99.893 % | Validation Accuracy=99.100 %\n",
      "   Epoch: 24 | Loss=0.004711753 | Train accuracy=99.656 % | Validation Accuracy=98.780 %\n",
      "   Epoch: 25 | Loss=0.007329930 | Train accuracy=99.836 % | Validation Accuracy=99.000 %\n",
      "   Epoch: 26 | Loss=0.006726557 | Train accuracy=99.907 % | Validation Accuracy=99.100 %\n",
      "   Epoch: 27 | Loss=0.004372639 | Train accuracy=99.976 % | Validation Accuracy=99.120 %\n",
      "   Epoch: 28 | Loss=0.004224668 | Train accuracy=99.751 % | Validation Accuracy=98.980 %\n",
      "   Epoch: 29 | Loss=0.005975527 | Train accuracy=99.716 % | Validation Accuracy=98.800 %\n",
      "   Epoch: 30 | Loss=0.008049639 | Train accuracy=99.907 % | Validation Accuracy=98.980 %\n",
      "   Epoch: 31 | Loss=0.003423009 | Train accuracy=99.904 % | Validation Accuracy=98.960 %\n",
      "   Epoch: 32 | Loss=0.002437323 | Train accuracy=99.896 % | Validation Accuracy=99.120 %\n",
      "   Epoch: 33 | Loss=0.003945075 | Train accuracy=99.958 % | Validation Accuracy=99.160 %\n",
      "   Epoch: 34 | Loss=0.002474558 | Train accuracy=99.318 % | Validation Accuracy=98.480 %\n",
      "   Epoch: 35 | Loss=0.007410054 | Train accuracy=99.835 % | Validation Accuracy=98.920 %\n",
      "   Epoch: 36 | Loss=0.006531725 | Train accuracy=99.829 % | Validation Accuracy=98.760 %\n",
      "   Epoch: 37 | Loss=0.003347021 | Train accuracy=99.855 % | Validation Accuracy=98.980 %\n",
      "   Epoch: 38 | Loss=0.003074192 | Train accuracy=99.862 % | Validation Accuracy=98.980 %\n",
      "   Epoch: 39 | Loss=0.003532632 | Train accuracy=99.949 % | Validation Accuracy=99.140 %\n",
      "   Epoch: 40 | Loss=0.003703682 | Train accuracy=99.918 % | Validation Accuracy=99.180 %\n",
      "   Training Finished in 1520.1 seconds.\n",
      "      \n",
      "Final accuracies:\n",
      "\n",
      "    * Train accuracy: 99.918 %\n",
      "    * Validation accuracy: 99.180 %\n",
      "    * Test accuracy: 99.050 %\n",
      "   Saving model in file: Models/lenet5-model-ADAM-lr=0.001\n"
     ]
    }
   ],
   "source": [
    "apply_le_net ( model=LeNet5_Model, modelName='lenet5-model-ADAM-lr=0.001',\n",
    "           learning_rate = 0.001, \n",
    "           training_epochs = 40,\n",
    "           batch_size = 128, \n",
    "           optimFunc = tf.train.AdamOptimizer\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model  lenet5-model-ADAM-lr=0.01  {learning rate: 0.0100; nb of iterations: 40; batch size: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=0.163237772 | Train accuracy=98.011 % | Validation Accuracy=97.980 %\n",
      "   Epoch: 02 | Loss=0.062061921 | Train accuracy=98.102 % | Validation Accuracy=97.760 %\n",
      "   Epoch: 03 | Loss=0.055803975 | Train accuracy=98.762 % | Validation Accuracy=98.480 %\n",
      "   Epoch: 04 | Loss=0.050317014 | Train accuracy=99.165 % | Validation Accuracy=98.680 %\n",
      "   Epoch: 05 | Loss=0.051566172 | Train accuracy=98.704 % | Validation Accuracy=98.380 %\n",
      "   Epoch: 06 | Loss=0.050115155 | Train accuracy=99.002 % | Validation Accuracy=98.500 %\n",
      "   Epoch: 07 | Loss=0.039589857 | Train accuracy=99.142 % | Validation Accuracy=98.440 %\n",
      "   Epoch: 08 | Loss=0.047223618 | Train accuracy=98.962 % | Validation Accuracy=98.420 %\n",
      "   Epoch: 09 | Loss=0.039301384 | Train accuracy=99.278 % | Validation Accuracy=98.720 %\n",
      "   Epoch: 10 | Loss=0.049096069 | Train accuracy=99.107 % | Validation Accuracy=98.520 %\n",
      "   Epoch: 11 | Loss=0.046008395 | Train accuracy=98.827 % | Validation Accuracy=98.360 %\n",
      "   Epoch: 12 | Loss=0.049097834 | Train accuracy=99.125 % | Validation Accuracy=98.660 %\n",
      "   Epoch: 13 | Loss=0.038264916 | Train accuracy=99.300 % | Validation Accuracy=98.720 %\n",
      "   Epoch: 14 | Loss=0.034690018 | Train accuracy=99.331 % | Validation Accuracy=98.780 %\n",
      "   Epoch: 15 | Loss=0.031846863 | Train accuracy=99.355 % | Validation Accuracy=98.740 %\n",
      "   Epoch: 16 | Loss=0.050365806 | Train accuracy=99.305 % | Validation Accuracy=98.580 %\n",
      "   Epoch: 17 | Loss=0.043051355 | Train accuracy=99.102 % | Validation Accuracy=98.380 %\n",
      "   Epoch: 18 | Loss=0.050732058 | Train accuracy=98.176 % | Validation Accuracy=97.720 %\n",
      "   Epoch: 19 | Loss=0.042747094 | Train accuracy=99.280 % | Validation Accuracy=98.440 %\n",
      "   Epoch: 20 | Loss=0.036178435 | Train accuracy=99.427 % | Validation Accuracy=98.720 %\n",
      "   Epoch: 21 | Loss=0.036663131 | Train accuracy=99.165 % | Validation Accuracy=98.620 %\n",
      "   Epoch: 22 | Loss=0.033113707 | Train accuracy=99.558 % | Validation Accuracy=98.840 %\n",
      "   Epoch: 23 | Loss=0.035731100 | Train accuracy=99.244 % | Validation Accuracy=98.540 %\n",
      "   Epoch: 24 | Loss=0.068579390 | Train accuracy=99.038 % | Validation Accuracy=98.400 %\n",
      "   Epoch: 25 | Loss=0.048864286 | Train accuracy=99.395 % | Validation Accuracy=98.760 %\n",
      "   Epoch: 26 | Loss=0.043205297 | Train accuracy=99.336 % | Validation Accuracy=98.620 %\n",
      "   Epoch: 27 | Loss=0.053024594 | Train accuracy=98.724 % | Validation Accuracy=97.880 %\n",
      "   Epoch: 28 | Loss=0.065499503 | Train accuracy=98.955 % | Validation Accuracy=98.060 %\n",
      "   Epoch: 29 | Loss=0.069380624 | Train accuracy=98.435 % | Validation Accuracy=97.740 %\n",
      "   Epoch: 30 | Loss=0.066413172 | Train accuracy=98.622 % | Validation Accuracy=97.860 %\n",
      "   Epoch: 31 | Loss=0.065773784 | Train accuracy=99.175 % | Validation Accuracy=98.680 %\n",
      "   Epoch: 32 | Loss=0.053155824 | Train accuracy=98.987 % | Validation Accuracy=98.160 %\n",
      "   Epoch: 33 | Loss=0.051637184 | Train accuracy=99.180 % | Validation Accuracy=98.440 %\n",
      "   Epoch: 34 | Loss=0.044910639 | Train accuracy=99.187 % | Validation Accuracy=98.360 %\n",
      "   Epoch: 35 | Loss=0.070266785 | Train accuracy=98.060 % | Validation Accuracy=97.360 %\n",
      "   Epoch: 36 | Loss=0.090692209 | Train accuracy=98.604 % | Validation Accuracy=98.120 %\n",
      "   Epoch: 37 | Loss=0.080126929 | Train accuracy=98.505 % | Validation Accuracy=97.940 %\n",
      "   Epoch: 38 | Loss=0.093796960 | Train accuracy=98.420 % | Validation Accuracy=98.080 %\n",
      "   Epoch: 39 | Loss=0.092647401 | Train accuracy=98.576 % | Validation Accuracy=98.000 %\n",
      "   Epoch: 40 | Loss=0.097701880 | Train accuracy=98.011 % | Validation Accuracy=97.620 %\n",
      "   Training Finished in 1521.0 seconds.\n",
      "      \n",
      "Final accuracies:\n",
      "\n",
      "    * Train accuracy: 98.011 %\n",
      "    * Validation accuracy: 97.620 %\n",
      "    * Test accuracy: 97.010 %\n",
      "   Saving model in file: Models/lenet5-model-ADAM-lr=0.01\n"
     ]
    }
   ],
   "source": [
    "apply_le_net (model=LeNet5_Model, modelName='lenet5-model-ADAM-lr=0.01', \n",
    "           learning_rate = 0.01, \n",
    "           training_epochs = 40,\n",
    "           batch_size = 128, \n",
    "           optimFunc = tf.train.AdamOptimizer\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "These are the results we have obtained for **`learning rate = 0.001`**\n",
    "\n",
    "| Optimizer                    |  Gradient Descent (learning rate = 0.001) |    AdamOptimizer (learning rate = 0.001) |\n",
    "|------------------------------|-----------------------|-----------------------|\n",
    "| Testing Accuracy             |         95.39 %       |       99.05 %         |       \n",
    "| Training Time (seconds)      |         1365.2        |       1520.1          |  \n",
    "\n",
    "\n",
    "---\n",
    "These are the results we have obtained for **`learning rate = 0.01`**\n",
    "\n",
    "| Optimizer                    |  Gradient Descent (learning rate = 0.01) |    AdamOptimizer (learning rate = 0.01) |\n",
    "|------------------------------|-----------------------|-----------------------|\n",
    "| Testing Accuracy             |         98.53 %       |       97.01 %         |       \n",
    "| Training Time (seconds)      |         1568.7        |       1521.0          |  \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<h3>ANSWER:</h3>\n",
    "<br>\n",
    "In both cases (both learning rates' values), the optimizer which gives the best accuracy on test data is <b>AdamOptimizer</b>. \n",
    "<br>\n",
    "ADAM algorithm both adapts the learning_rate to the parameters, and add momentum (accelerate the gradient in the relevant direction while reducing oscillations).\n",
    "<br>\n",
    "Therefore, for the next question, we will keep this optimizer instead of Gradient Descent.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the curves of the accuracy and the loss obtained from Tensorboard:\n",
    "### With `learning_rate 0.001`\n",
    "![alt text](MNIST_figures/final-Adam-001.png \"Tensorboard curves - ADAM-0.001\")\n",
    "### With `learning_rate 0.01`\n",
    "![alt text](MNIST_figures/final-Adam-01.png \"Tensorboard curves - ADAM-0.01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2</b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use tf.nn.dropout for that purpose. What accuracy do you achieve on testing data?\n",
    "\n",
    "**Accuracy achieved on testing data:** 99.24 % !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model_Dropout(data, keep_prob=0.75):\n",
    "    input_data = tf.reshape(data,[-1,28,28,1])\n",
    "    \n",
    "    # Layer 1: Convolution / Input shape = 28x28x1 / Output shape = 28x28x6\n",
    "    w1 = weight_variable([5,5,1,6])\n",
    "    b1 = bias_variable([6])\n",
    "    conv1 = tf.nn.relu(conv2d(input_data , w1) + b1) #activation\n",
    "    \n",
    "    # Max Pooling / Input shape = 28x28x6 / Output shape = 14x14x6\n",
    "    pool1 = max_pool_size2(conv1)\n",
    "    \n",
    "    # Layer 2: Convolutional / Input shape = 14*14*6 / Output shape = 10x10x16\n",
    "    w2 = weight_variable([5,5,6,16])\n",
    "    b2 = bias_variable([16])\n",
    "    conv2 = tf.nn.relu(conv2d( pool1  ,  w2  ,  pad='VALID') + b2) # activation\n",
    "    \n",
    "    # Max Pooling / Input shape = 10x10x16 / Output shape = 5x5x16\n",
    "    pool2 = max_pool_size2(conv2)\n",
    "    \n",
    "    # add dropout here\n",
    "    drop_pool2 = tf.nn.dropout(pool2, keep_prob)\n",
    "    \n",
    "    # Flatten the output shape / Input shape = 5x5x16 / Output shape = 400\n",
    "    pool2_flat = tf.contrib.layers.flatten(drop_pool2)\n",
    "    \n",
    "    # Layer 3: Fully Connected / Input shape = 400 / Output shape = 120\n",
    "    w3 = weight_variable([400, 120])\n",
    "    b3 = bias_variable([120])\n",
    "    fc1 = tf.nn.relu(tf.matmul(pool2_flat, w3) + b3) # activation\n",
    "    \n",
    "    # Layer 4: Fully Connected / Input shape = 120 / Output shape = 84\n",
    "    w4 = weight_variable([120, 84])\n",
    "    b4 = bias_variable([84])\n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, w4) + b4) # activation\n",
    "    \n",
    "    # Layer 5: Fully Connected / Input shape = 84 / Output shape = 10\n",
    "    w5 = weight_variable([84, 10])\n",
    "    b5 = bias_variable([10])\n",
    "    \n",
    "    pred = tf.nn.softmax(tf.matmul(fc2, w5) + b5) # softmax activation\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model  lenet5-model-ADAM-Dropout-lr=0.001  {learning rate: 0.0010; nb of iterations: 40; batch size: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=0.324561476 | Train accuracy=97.049 % | Validation Accuracy=97.440 %\n",
      "   Epoch: 02 | Loss=0.079186635 | Train accuracy=98.095 % | Validation Accuracy=98.220 %\n",
      "   Epoch: 03 | Loss=0.055595786 | Train accuracy=98.527 % | Validation Accuracy=98.460 %\n",
      "   Epoch: 04 | Loss=0.046442755 | Train accuracy=98.595 % | Validation Accuracy=98.260 %\n",
      "   Epoch: 05 | Loss=0.035803478 | Train accuracy=99.180 % | Validation Accuracy=98.960 %\n",
      "   Epoch: 06 | Loss=0.032174475 | Train accuracy=99.136 % | Validation Accuracy=98.740 %\n",
      "   Epoch: 07 | Loss=0.027366817 | Train accuracy=99.315 % | Validation Accuracy=98.880 %\n",
      "   Epoch: 08 | Loss=0.023349417 | Train accuracy=99.147 % | Validation Accuracy=98.700 %\n",
      "   Epoch: 09 | Loss=0.022840873 | Train accuracy=99.347 % | Validation Accuracy=98.900 %\n",
      "   Epoch: 10 | Loss=0.018556138 | Train accuracy=99.420 % | Validation Accuracy=98.700 %\n",
      "   Epoch: 11 | Loss=0.016374677 | Train accuracy=99.707 % | Validation Accuracy=98.860 %\n",
      "   Epoch: 12 | Loss=0.013356575 | Train accuracy=99.595 % | Validation Accuracy=98.860 %\n",
      "   Epoch: 13 | Loss=0.011948840 | Train accuracy=99.636 % | Validation Accuracy=99.000 %\n",
      "   Epoch: 14 | Loss=0.012069453 | Train accuracy=99.631 % | Validation Accuracy=98.920 %\n",
      "   Epoch: 15 | Loss=0.010665578 | Train accuracy=99.749 % | Validation Accuracy=99.040 %\n",
      "   Epoch: 16 | Loss=0.009844423 | Train accuracy=99.691 % | Validation Accuracy=99.000 %\n",
      "   Epoch: 17 | Loss=0.008450624 | Train accuracy=99.640 % | Validation Accuracy=98.820 %\n",
      "   Epoch: 18 | Loss=0.008689516 | Train accuracy=99.793 % | Validation Accuracy=98.960 %\n",
      "   Epoch: 19 | Loss=0.007636576 | Train accuracy=99.740 % | Validation Accuracy=99.000 %\n",
      "   Epoch: 20 | Loss=0.008962386 | Train accuracy=99.480 % | Validation Accuracy=99.060 %\n",
      "   Epoch: 21 | Loss=0.006560727 | Train accuracy=99.778 % | Validation Accuracy=99.000 %\n",
      "   Epoch: 22 | Loss=0.005805614 | Train accuracy=99.776 % | Validation Accuracy=98.960 %\n",
      "   Epoch: 23 | Loss=0.006432501 | Train accuracy=99.844 % | Validation Accuracy=99.020 %\n",
      "   Epoch: 24 | Loss=0.007817652 | Train accuracy=99.685 % | Validation Accuracy=98.860 %\n",
      "   Epoch: 25 | Loss=0.004559416 | Train accuracy=99.822 % | Validation Accuracy=99.060 %\n",
      "   Epoch: 26 | Loss=0.005472120 | Train accuracy=99.905 % | Validation Accuracy=99.020 %\n",
      "   Epoch: 27 | Loss=0.004718746 | Train accuracy=99.875 % | Validation Accuracy=98.900 %\n",
      "   Epoch: 28 | Loss=0.005296614 | Train accuracy=99.938 % | Validation Accuracy=99.060 %\n",
      "   Epoch: 29 | Loss=0.005052068 | Train accuracy=99.944 % | Validation Accuracy=98.880 %\n",
      "   Epoch: 30 | Loss=0.005213639 | Train accuracy=99.880 % | Validation Accuracy=99.020 %\n",
      "   Epoch: 31 | Loss=0.004009379 | Train accuracy=99.951 % | Validation Accuracy=98.960 %\n",
      "   Epoch: 32 | Loss=0.004021918 | Train accuracy=99.940 % | Validation Accuracy=99.200 %\n",
      "   Epoch: 33 | Loss=0.004855093 | Train accuracy=99.953 % | Validation Accuracy=99.140 %\n",
      "   Epoch: 34 | Loss=0.003788066 | Train accuracy=99.964 % | Validation Accuracy=99.080 %\n",
      "   Epoch: 35 | Loss=0.001779752 | Train accuracy=99.938 % | Validation Accuracy=99.280 %\n",
      "   Epoch: 36 | Loss=0.004410580 | Train accuracy=99.945 % | Validation Accuracy=99.100 %\n",
      "   Epoch: 37 | Loss=0.006375885 | Train accuracy=99.831 % | Validation Accuracy=99.000 %\n",
      "   Epoch: 38 | Loss=0.004174035 | Train accuracy=99.871 % | Validation Accuracy=99.120 %\n",
      "   Epoch: 39 | Loss=0.001527143 | Train accuracy=99.996 % | Validation Accuracy=99.220 %\n",
      "   Epoch: 40 | Loss=0.000533116 | Train accuracy=100.000 % | Validation Accuracy=99.300 %\n",
      "   Training Finished in 1622.8 seconds.\n",
      "      \n",
      "Final accuracies:\n",
      "\n",
      "    * Train accuracy: 100.000 %\n",
      "    * Validation accuracy: 99.300 %\n",
      "    * Test accuracy: 99.240 %\n",
      "   Saving model in file: Models/lenet5-model-ADAM-Dropout-lr=0.001\n"
     ]
    }
   ],
   "source": [
    "apply_le_net ('lenet5-model-ADAM-Dropout-lr=0.001', LeNet5_Model_Dropout,\n",
    "           learning_rate = 0.001, \n",
    "           training_epochs = 40,\n",
    "           batch_size = 128, \n",
    "           optimFunc = tf.train.AdamOptimizer\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model  lenet5-model-ADAM-Dropout-lr=0.01  {learning rate: 0.0100; nb of iterations: 40; batch size: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=0.163451382 | Train accuracy=98.120 % | Validation Accuracy=98.000 %\n",
      "   Epoch: 02 | Loss=0.069643003 | Train accuracy=97.780 % | Validation Accuracy=97.940 %\n",
      "   Epoch: 03 | Loss=0.058219914 | Train accuracy=98.511 % | Validation Accuracy=98.360 %\n",
      "   Epoch: 04 | Loss=0.054814242 | Train accuracy=98.847 % | Validation Accuracy=98.640 %\n",
      "   Epoch: 05 | Loss=0.048273921 | Train accuracy=98.549 % | Validation Accuracy=98.220 %\n",
      "   Epoch: 06 | Loss=0.045832736 | Train accuracy=98.676 % | Validation Accuracy=98.200 %\n",
      "   Epoch: 07 | Loss=0.052985071 | Train accuracy=98.687 % | Validation Accuracy=98.280 %\n",
      "   Epoch: 08 | Loss=0.046835801 | Train accuracy=98.831 % | Validation Accuracy=98.500 %\n",
      "   Epoch: 09 | Loss=0.042724919 | Train accuracy=99.056 % | Validation Accuracy=98.520 %\n",
      "   Epoch: 10 | Loss=0.043842827 | Train accuracy=99.109 % | Validation Accuracy=98.380 %\n",
      "   Epoch: 11 | Loss=0.048297016 | Train accuracy=99.165 % | Validation Accuracy=98.440 %\n",
      "   Epoch: 12 | Loss=0.051403085 | Train accuracy=98.973 % | Validation Accuracy=98.160 %\n",
      "   Epoch: 13 | Loss=0.048661624 | Train accuracy=98.895 % | Validation Accuracy=98.140 %\n",
      "   Epoch: 14 | Loss=0.042540837 | Train accuracy=99.244 % | Validation Accuracy=98.700 %\n",
      "   Epoch: 15 | Loss=0.032694515 | Train accuracy=99.245 % | Validation Accuracy=98.520 %\n",
      "   Epoch: 16 | Loss=0.041668322 | Train accuracy=99.022 % | Validation Accuracy=97.980 %\n",
      "   Epoch: 17 | Loss=0.033693202 | Train accuracy=99.549 % | Validation Accuracy=98.520 %\n",
      "   Epoch: 18 | Loss=0.039651960 | Train accuracy=99.425 % | Validation Accuracy=98.420 %\n",
      "   Epoch: 19 | Loss=0.030281927 | Train accuracy=99.484 % | Validation Accuracy=98.560 %\n",
      "   Epoch: 20 | Loss=0.034413874 | Train accuracy=99.251 % | Validation Accuracy=98.440 %\n",
      "   Epoch: 21 | Loss=0.075061922 | Train accuracy=98.944 % | Validation Accuracy=98.240 %\n",
      "   Epoch: 22 | Loss=0.050744452 | Train accuracy=99.420 % | Validation Accuracy=98.940 %\n",
      "   Epoch: 23 | Loss=0.030243595 | Train accuracy=98.824 % | Validation Accuracy=98.100 %\n",
      "   Epoch: 24 | Loss=0.038360344 | Train accuracy=98.951 % | Validation Accuracy=98.020 %\n",
      "   Epoch: 25 | Loss=0.054260410 | Train accuracy=98.549 % | Validation Accuracy=97.620 %\n",
      "   Epoch: 26 | Loss=0.055832272 | Train accuracy=99.293 % | Validation Accuracy=98.600 %\n",
      "   Epoch: 27 | Loss=0.060575641 | Train accuracy=99.040 % | Validation Accuracy=98.400 %\n",
      "   Epoch: 28 | Loss=0.051136414 | Train accuracy=98.647 % | Validation Accuracy=97.660 %\n",
      "   Epoch: 29 | Loss=0.054171635 | Train accuracy=99.275 % | Validation Accuracy=98.380 %\n",
      "   Epoch: 30 | Loss=0.054214647 | Train accuracy=98.513 % | Validation Accuracy=97.600 %\n",
      "   Epoch: 31 | Loss=0.086520154 | Train accuracy=98.322 % | Validation Accuracy=97.680 %\n",
      "   Epoch: 32 | Loss=0.089907839 | Train accuracy=97.704 % | Validation Accuracy=97.420 %\n",
      "   Epoch: 33 | Loss=0.121364516 | Train accuracy=89.080 % | Validation Accuracy=88.580 %\n",
      "   Epoch: 34 | Loss=0.102892571 | Train accuracy=97.653 % | Validation Accuracy=96.780 %\n",
      "   Epoch: 35 | Loss=0.159820636 | Train accuracy=93.800 % | Validation Accuracy=93.300 %\n",
      "   Epoch: 36 | Loss=0.240855887 | Train accuracy=95.285 % | Validation Accuracy=95.220 %\n",
      "   Epoch: 37 | Loss=0.186038543 | Train accuracy=96.245 % | Validation Accuracy=96.340 %\n",
      "   Epoch: 38 | Loss=0.231813530 | Train accuracy=96.471 % | Validation Accuracy=96.440 %\n",
      "   Epoch: 39 | Loss=0.312322154 | Train accuracy=94.993 % | Validation Accuracy=94.880 %\n",
      "   Epoch: 40 | Loss=0.260834967 | Train accuracy=95.038 % | Validation Accuracy=95.040 %\n",
      "   Training Finished in 1523.0 seconds.\n",
      "      \n",
      "Final accuracies:\n",
      "\n",
      "    * Train accuracy: 95.038 %\n",
      "    * Validation accuracy: 95.040 %\n",
      "    * Test accuracy: 94.710 %\n",
      "   Saving model in file: Models/lenet5-model-ADAM-Dropout-lr=0.01\n"
     ]
    }
   ],
   "source": [
    "apply_le_net ('lenet5-model-ADAM-Dropout-lr=0.01', LeNet5_Model_Dropout,\n",
    "           learning_rate = 0.01, \n",
    "           training_epochs = 40,\n",
    "           batch_size = 128, \n",
    "           optimFunc = tf.train.AdamOptimizer\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the curves of the accuracy and the loss obtained from Tensorboard:\n",
    "### Learning_rate 0.001\n",
    "![alt text](MNIST_figures/final-Adam-dropout-001.png \"Tensorboard curves - ADAM and dropout - 0.001\")\n",
    "### Learning_rate 0.01\n",
    "![alt text](MNIST_figures/final-Adam-dropout-01.png \"Tensorboard curves - ADAM and dropout - 0.01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3>Conclusion</h3><br>\n",
    "<ul>\n",
    "<li>Generally speaking LeNet-5 classifies very accurately the images (accuracy above 95%!).</li>\n",
    "<li>Firstly, we used Gradient Descent optimization, but then to improve performance we used a better optimizer: ADAM.\n",
    "We reached 99.05 % accuracy on test data.</li>\n",
    "<li>Secondly, in order to avoid overfitting we introduced dropout. This leads to a more accurate classification (99.24% at maximum), and it is a strong caution not to overfit.</li>\n",
    "<li>Moreover we compared the result for 2 different learning-rates: 0.01 and 0.001. At first we thought a 0.01 learning-rate would give better results (since it's the case for Gradient Descent optimizer), but then it turns out to be less accurate with ADAM optimizer and dropout, while 0.001 gave us more than 99 % accuracy.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
